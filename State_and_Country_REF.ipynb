{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c247ceb4-3838-414c-a816-9279fa07186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rerunning Terry's code to clean the dataset in the same way he already did\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "#collect all csvs in one list to read them all into one dataframe\n",
    "csv_urls = [\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/jan2005-jun2005.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/jul2005-mar2006.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/april2006-november2006.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/december2006-august2007.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/september2007-may2008.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/june2008-february2009.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/march2009-november2009.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/december2009-august2010.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/september2010-may2011.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/june2011-february2012.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/march2012-november2012.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/december2012-august2013.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/september2013-may2014.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/june2014-april2015.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/may2015-january2016.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/february2016-december2016.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/january2017-november2017.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/december2017-october2018.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/november2018-july2019.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/august2019-may2020.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/june2020-april2021.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/may2021-apr2022.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/may2022-mar2023.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/apr2023-mar2024.csv',\n",
    "    'https://github.com/terryljh/aviation2/raw/refs/heads/main/ASRS%20data/apr2024-dec2025.csv']\n",
    "    #this code usually takes about 15-30 seconds to read in all csv's\n",
    "df = pd.concat([pd.read_csv(url) for url in csv_urls], ignore_index=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "new_headers = df.iloc[0] #dataset has headers, then the first row is the actual headings,\n",
    "#so we want to replace headers with the first row new_headers\n",
    "\n",
    "#removes anomolous dates that are in by mistake, should only be April 2024-May 2025\n",
    "\n",
    "# Make the headers unique, in case some are repeated, function relabels 2,3 etc\n",
    "#The ASRS occasionally has multiple reports for a single incident\n",
    "#If we want to include second,third reports in our data, we need to include the\n",
    "#second version of those columns too, but for simplicity maybe we only take the\n",
    "#first report of each incident for now?\n",
    "def make_unique(headers):\n",
    "    counts = {}\n",
    "    unique_headers = []\n",
    "    for col in headers:\n",
    "        if col in counts:\n",
    "            counts[col] += 1\n",
    "            unique_headers.append(f\"{col}_{counts[col]}\")\n",
    "        else:\n",
    "            counts[col] = 0\n",
    "            unique_headers.append(col)\n",
    "    return unique_headers\n",
    "unique_headers = make_unique(new_headers)\n",
    "\n",
    "df.columns = unique_headers\n",
    "df = df.iloc[1:].reset_index(drop=True) #remove unnecessary first row\n",
    "\n",
    "df['Date']= df['Date'].values\n",
    "df['Date'] = pd.to_numeric(df['Date'], errors='coerce')\n",
    "#the date column is of type string, we change to integer so we can manipulate it\n",
    "#errors='coerce' will return NaN if not a number\n",
    "dfsubset = df[['Date', 'Locale Reference', 'State Reference', 'Operating Under FAR Part','Mission','Light', 'Flight Phase','Function', 'Contributing Factors / Situations', 'Primary Problem']]\n",
    "dfsubset=dfsubset.copy()\n",
    "dfsubset = dfsubset[(dfsubset['Date']//100 >= 2004) & (dfsubset['Date']//100 <= 2026)]\n",
    "# there are accidentally some lines where the year is 0 BC, so we remove them#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8514561f-4ecc-4652-9bbe-51ca789f01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make normalized column from state ref column  where:\n",
    "# make all upper case \n",
    "# strip leading or trailing white space\n",
    "\n",
    "df['Normalized'] = df['State Reference'].str.upper().str.strip()\n",
    "\n",
    "#Define U.S. state abbreviations (includes DC)\n",
    "us_states = {\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA',\n",
    "    'HI', 'ID', 'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD',\n",
    "    'MA', 'MI', 'MN', 'MS', 'MO', 'MT', 'NE', 'NV', 'NH', 'NJ',\n",
    "    'NM', 'NY', 'NC', 'ND', 'OH', 'OK', 'OR', 'PA', 'RI', 'SC',\n",
    "    'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV', 'WI', 'WY', 'DC'\n",
    "}\n",
    "\n",
    "#Define set of ISO alpha-2 country codes\n",
    "country_codes = { \n",
    "    'AD', 'AE', 'AF', 'AG', 'AI', 'AL', 'AM', 'AO', 'AQ', 'AR', 'AS', 'AT', 'AU', 'AW', 'AX', 'AZ',\n",
    "    'BA', 'BB', 'BD', 'BE', 'BF', 'BG', 'BH', 'BI', 'BJ', 'BL', 'BM', 'BN', 'BO', 'BQ', 'BR', 'BS', 'BT', 'BV', 'BW', 'BY', 'BZ',\n",
    "    'CA', 'CC', 'CD', 'CF', 'CG', 'CH', 'CI', 'CK', 'CL', 'CM', 'CN', 'CO', 'CR', 'CU', 'CV', 'CW', 'CX', 'CY', 'CZ',\n",
    "    'DE', 'DJ', 'DK', 'DM', 'DO', 'DZ', 'EC', 'EE', 'EG', 'EH', 'ER', 'ES', 'ET',\n",
    "    'FI', 'FJ', 'FM', 'FO', 'FR','GA', 'GB', 'GD', 'GE', 'GF', 'GG', 'GH', 'GI', 'GL', 'GM', 'GN', 'GP', 'GQ', 'GR', 'GT', 'GU', 'GW', 'GY',\n",
    "    'HK', 'HM', 'HN', 'HR', 'HT', 'HU', 'ID', 'IE', 'IL', 'IM', 'IN', 'IO', 'IQ', 'IR', 'IS', 'IT',\n",
    "    'JE', 'JM', 'JO', 'JP','KE', 'KG', 'KH', 'KI', 'KM', 'KN', 'KP', 'KR', 'KW', 'KY', 'KZ',\n",
    "    'LA', 'LB', 'LC', 'LI', 'LK', 'LR', 'LS', 'LT', 'LU', 'LV', 'LY',\n",
    "    'MA', 'MC', 'MD', 'ME', 'MF', 'MG', 'MH', 'MK', 'ML', 'MM', 'MN', 'MO', 'MP', 'MQ', 'MR', 'MS', 'MT', 'MU', 'MV', 'MW', 'MX', 'MY', 'MZ',\n",
    "    'NA', 'NC', 'NE', 'NF', 'NG', 'NI', 'NL', 'NO', 'NP', 'NR', 'NU', 'NZ', 'OM','PA', 'PE', 'PF', 'PG', 'PH', 'PK', 'PL', 'PM', 'PN', 'PR', 'PT', 'PW', 'PY',\n",
    "    'QA', 'RE', 'RO', 'RS', 'RU', 'RW', 'SA', 'SB', 'SC', 'SD', 'SE', 'SG', 'SH', 'SI', 'SJ', 'SK', 'SL', 'SM', 'SN', 'SO', 'SR', 'SS', 'ST', 'SV', 'SX', 'SY', 'SZ',\n",
    "    'TC', 'TD', 'TF', 'TG', 'TH', 'TJ', 'TK', 'TL', 'TM', 'TN', 'TO', 'TR', 'TT', 'TV', 'TZ',\n",
    "    'UA', 'UG', 'UM', 'US', 'UY', 'UZ', 'VA', 'VC', 'VE', 'VG', 'VI', 'VN', 'VU', 'WF', 'WS', 'YE', 'YT', 'ZA', 'ZM', 'ZW'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da027bb5-0f9b-4160-9290-fad3a2bdfbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import pandas as pd\n",
    "\n",
    "# make a copy of cleaned data\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "#  Initial classification of State, Country or 'State or Country' or Unknown\n",
    "df_cleaned['Is_State'] = df_cleaned['Normalized'].isin(us_states)\n",
    "df_cleaned['Is_Country'] = df_cleaned['Normalized'].isin(country_codes)\n",
    "df_cleaned['Is_Ambiguous'] = df_cleaned['Is_State'] & df_cleaned['Is_Country']\n",
    "\n",
    "def classify_abbreviation(row):\n",
    "    if row['Is_Ambiguous']:\n",
    "        return 'State or Country'\n",
    "    elif row['Is_State']:\n",
    "        return 'State'\n",
    "    elif row['Is_Country']:\n",
    "        return 'Country'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "df_cleaned['Abbreviation_Type'] = df_cleaned.apply(classify_abbreviation, axis=1)\n",
    "\n",
    "# Filter only valid 2-character abbreviations\n",
    "df_cleaned['Length_Valid'] = df_cleaned['Normalized'].str.len() == 2\n",
    "df_final = df_cleaned[df_cleaned['Length_Valid']].copy()\n",
    "\n",
    "# Load airport codes from FreightPaul featured freight airport code system (IATA)\n",
    "airport_codes = pd.read_csv(\"C:/Users/steph/Downloads/airport_codes.csv\")\n",
    "\n",
    "# Extract 3-letter airport code from Locale Reference\n",
    "df_final.loc[:, 'Airport_Code'] = df_final['Locale Reference'].str[:3]\n",
    "\n",
    "# Update ambiguous or unknown using airport data\n",
    "needs_update = df_final['Abbreviation_Type'].isin(['Unknown', 'State or Country'])\n",
    "df_to_update = df_final[needs_update].copy()\n",
    "\n",
    "df_to_update = df_to_update.merge(\n",
    "    airport_codes[['abrv', 'Code']],\n",
    "    left_on='Airport_Code',\n",
    "    right_on='abrv',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "def resolve_from_airport(row):\n",
    "    if pd.isna(row['Code']):\n",
    "        return row['Abbreviation_Type']\n",
    "    elif row['Code'] == 'US':\n",
    "        return 'State'\n",
    "    else:\n",
    "        return 'Country'\n",
    "\n",
    "df_to_update['Resolved_Type'] = df_to_update.apply(resolve_from_airport, axis=1)\n",
    "\n",
    "# Update back into df_final\n",
    "for idx, row in df_to_update.iterrows():\n",
    "    df_final.loc[idx, 'Abbreviation_Type'] = row['Resolved_Type']\n",
    "\n",
    "# Safeguard â€” always treat 'US' as a country\n",
    "df_final.loc[df_final['Normalized'] == 'US', 'Abbreviation_Type'] = 'Country'\n",
    "df_final.loc[df_final['State Reference'].str.strip().str.upper() == 'US', 'Abbreviation_Type'] = 'Country'\n",
    "\n",
    "# apply fallback logic to any rows still ambiguous\n",
    "still_uncertain = df_final['Abbreviation_Type'].isin(['State or Country', 'Unknown'])\n",
    "\n",
    "def final_disambiguation(code):\n",
    "    if code in us_states:\n",
    "        return 'State'\n",
    "    elif code in country_codes:\n",
    "        return 'Country'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "df_final.loc[still_uncertain, 'Abbreviation_Type'] = df_final.loc[still_uncertain, 'Normalized'].apply(final_disambiguation)\n",
    "\n",
    "# Save result\n",
    "output_path = \"C:/Users/steph/Downloads/final_state_country_with_airports.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "print(f\" Final cleaned file saved to: {output_path}\")\n",
    "\n",
    "# Summary\n",
    "print(df_final['Abbreviation_Type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8add0eba-01d5-4001-9898-7749a2a045d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#there were some Canadian abbrevations that weren't in the original country codes so they were made out to all be labeled as \"Country\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define Canadian province abbreviations to treat as countries\n",
    "canadian_provinces = {'ON', 'AB', 'BC', 'PQ', 'NB', 'MB', 'NS'}\n",
    "\n",
    "file_path = \"C:/Users/steph/Downloads/final_state_country_with_airports.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Normalize again just in case\n",
    "df['Normalized'] = df['Normalized'].str.upper().str.strip()\n",
    "\n",
    "df.loc[df['Normalized'].isin(canadian_provinces), 'Abbreviation_Type'] = 'Country'\n",
    "\n",
    "df.to_csv(file_path, index=False)\n",
    "print(f\" Updated Canadian provinces labeled as 'Country' and saved to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707fb60-410e-427f-9fe7-d92529615291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:erdos_summer_2025]",
   "language": "python",
   "name": "conda-env-erdos_summer_2025-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
